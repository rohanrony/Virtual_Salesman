{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "!pip install google-generativeai --upgrade\n",
        "!pip install openai\n",
        "!pip install elevenlabs\n",
        "!pip install pydub\n",
        "!pip install webrtcvad\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZyo1I2_F8B1",
        "outputId": "e47d77dc-490c-4db6-d470-af40aa76cac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.164.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.10.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.69.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.27.2)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.68.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Requirement already satisfied: elevenlabs in /usr/local/lib/python3.11/dist-packages (1.54.0)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from elevenlabs) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from elevenlabs) (2.10.6)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from elevenlabs) (2.27.2)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from elevenlabs) (2.32.3)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from elevenlabs) (4.12.2)\n",
            "Requirement already satisfied: websockets>=11.0 in /usr/local/lib/python3.11/dist-packages (from elevenlabs) (15.0.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->elevenlabs) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->elevenlabs) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->elevenlabs) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->elevenlabs) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.21.2->elevenlabs) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9.2->elevenlabs) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->elevenlabs) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->elevenlabs) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.21.2->elevenlabs) (1.3.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: webrtcvad in /usr/local/lib/python3.11/dist-packages (2.0.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hNOAlH3FBlp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "import openai\n",
        "import elevenlabs\n",
        "from elevenlabs import play\n",
        "from google.colab import userdata\n",
        "# import google.generativeai as genai\n",
        "import google.generativeai as genai\n",
        "\n",
        "import io\n",
        "import time\n",
        "import tempfile\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "import io\n",
        "from IPython.display import Audio, display, Javascript\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import detect_nonsilent\n",
        "# First cell in Colab notebook\n",
        "from google.colab import output\n",
        "from IPython.display import Javascript, HTML, display"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uHrll_oU63ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript, Audio\n",
        "from google.colab import output\n",
        "import webrtcvad\n",
        "import numpy as np\n",
        "from base64 import b64decode, b64encode\n",
        "import io\n",
        "\n",
        "# Modified recording function with VAD\n",
        "def record_audio(seconds_max=30, vad_mode=3, silence_threshold=1.5):\n",
        "    # JavaScript for recording\n",
        "    js_code = \"\"\"\n",
        "    // JavaScript code for recording with silence detection\n",
        "    const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "    const b2text = blob => new Promise(resolve => {\n",
        "        const reader = new FileReader()\n",
        "        reader.onloadend = e => resolve(e.srcElement.result)\n",
        "        reader.readAsDataURL(blob)\n",
        "    })\n",
        "\n",
        "    async function record_with_vad(time_max) {\n",
        "        stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "        recorder = new MediaRecorder(stream)\n",
        "        chunks = []\n",
        "\n",
        "        recorder.ondataavailable = e => chunks.push(e.data)\n",
        "        recorder.start(100) // Collect data in small chunks for analysis\n",
        "\n",
        "        const startTime = Date.now()\n",
        "        let silenceStart = null\n",
        "        let silenceThreshold = 1500 // 1.5 seconds in milliseconds\n",
        "\n",
        "        // Create audio context for analysis\n",
        "        const audioContext = new AudioContext()\n",
        "        const analyser = audioContext.createAnalyser()\n",
        "        const microphone = audioContext.createMediaStreamSource(stream)\n",
        "        microphone.connect(analyser)\n",
        "        analyser.fftSize = 512\n",
        "        const bufferLength = analyser.frequencyBinCount\n",
        "        const dataArray = new Uint8Array(bufferLength)\n",
        "\n",
        "        while ((Date.now() - startTime) < time_max * 1000) {\n",
        "            // Analyze audio\n",
        "            analyser.getByteFrequencyData(dataArray)\n",
        "            let sum = dataArray.reduce((a, b) => a + b, 0)\n",
        "            let average = sum / bufferLength\n",
        "\n",
        "            if (average < 10) { // Low audio level threshold\n",
        "                if (!silenceStart) {\n",
        "                    silenceStart = Date.now()\n",
        "                } else if ((Date.now() - silenceStart) > silenceThreshold) {\n",
        "                    console.log(\"Silence detected, stopping\")\n",
        "                    break\n",
        "                }\n",
        "            } else {\n",
        "                silenceStart = null\n",
        "            }\n",
        "\n",
        "            await sleep(100)\n",
        "        }\n",
        "\n",
        "        recorder.stop()\n",
        "        await sleep(100) // Give time for ondataavailable to fire\n",
        "\n",
        "        blob = new Blob(chunks)\n",
        "        text = await b2text(blob)\n",
        "        return text\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Recording... (speak naturally, recording will stop after silence)\")\n",
        "    display(Javascript(js_code))\n",
        "    audio_data_b64 = output.eval_js(f'record_with_vad({seconds_max})')\n",
        "    print(\"Recording finished.\")\n",
        "\n",
        "    # Decode the base64 audio data\n",
        "    audio_binary = b64decode(audio_data_b64.split(',')[1])\n",
        "\n",
        "    # Return both binary data and Audio widget\n",
        "    return audio_binary#, Audio(audio_binary)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xh8l73bDY4sK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize Gemini\n",
        "#client = genai.Client(api_key=userdata.get('GEMINI_API_KEY'))\n",
        "\n",
        "#model = genai.GenerativeModel('gemini-pro')\n",
        "#model = genai('models/gemini-pro')\n",
        "\n",
        "# Function to generate and play speech\n",
        "def speak(text):\n",
        "    # Instantiate ElevenLabs with your API key\n",
        "    elevenlabs_instance = elevenlabs.ElevenLabs(api_key=userdata.get('ELEVENLABS_API_KEY'))\n",
        "\n",
        "    # Generate and play audio using the instance\n",
        "    audio_stream = elevenlabs_instance.generate(\n",
        "        text=text,\n",
        "        voice=\"Antoni\",\n",
        "        model=\"eleven_monolingual_v1\"\n",
        "    )\n",
        "\n",
        "    play(audio_stream, elevenlabs_instance)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def speak(text):\n",
        "    # Instantiate ElevenLabs with your API key\n",
        "    elevenlabs_instance = elevenlabs.ElevenLabs(api_key=userdata.get('ELEVENLABS_API_KEY'))\n",
        "\n",
        "    # Generate and play audio using the instance\n",
        "    audio_stream = elevenlabs_instance.generate(\n",
        "        text=text,\n",
        "        voice=\"Antoni\",\n",
        "        model=\"eleven_monolingual_v1\",\n",
        "        stream=True\n",
        "    )\n",
        "\n",
        "    audio_data = io.BytesIO()\n",
        "    for chunk in audio_stream:\n",
        "        audio_data.write(chunk)\n",
        "\n",
        "    audio_data.seek(0)\n",
        "    play(audio_data.getvalue(),elevenlabs_instance)\n",
        "\n",
        "    # Calculate duration of audio (assuming 24kHz sample rate and 16-bit audio)\n",
        "    duration = len(audio_data.getvalue()) / (24000)  # in seconds\n",
        "\n",
        "    return duration\n",
        "\n",
        "\n",
        "import io\n",
        "import tempfile\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "def transcribe(audio_data):\n",
        "    # Create a temporary file to store the audio data\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as temp_audio_file:\n",
        "        # Write the audio data to the temporary file\n",
        "        temp_audio_file.write(audio_data)\n",
        "        temp_audio_file.flush()\n",
        "\n",
        "        # Rewind the file pointer to the beginning\n",
        "        temp_audio_file.seek(0)\n",
        "\n",
        "        # Perform transcription using the OpenAI API\n",
        "        # Pass the file path to the 'file' argument instead of the file object\n",
        "        transcript = client.audio.transcriptions.create(\n",
        "            model=\"whisper-1\",\n",
        "            file=open(temp_audio_file.name, \"rb\") # Open the file in binary read mode\n",
        "        )\n",
        "\n",
        "    return transcript.text\n",
        "\n",
        "'''\n",
        "# Function to get AI response\n",
        "def get_ai_response(prompt):\n",
        "    # Use client to generate content instead of the undefined 'model' variable.\n",
        "    # Using gemini-2.0-flash model, but consider using other available models.\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.0-flash-lite\",  # Replace with desired model\n",
        "        contents=prompt\n",
        "    )\n",
        "    return response.text\n",
        "'''\n",
        "# Configure the API\n",
        "genai.configure(api_key=userdata.get('GEMINI_API_KEY'))\n",
        "\n",
        "def get_ai_response(prompt):\n",
        "    model = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "vABVyzoMFCiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main conversation loop\n",
        "\n",
        "context = \"Elfi-homes. We offer regular system check that includes visual inspection of components, thermal imaging and cleaning as required. We recommend to use this making sure system is performing to its capabilities. 2 Checks/Year is recommended. We also offer installing EV charging station, Main Panel Upgrades and installing electric appliances. For the time being, we are offering first 4 inspections for free. The package cost is $1000 worth.   The package includes free cleaning and free system health check. It is important to keep an eye on the batteries and also making sure your solar system is producing proper electricity.\"\n",
        "\n",
        "# Your AI response generation here\n",
        "#ai_response = get_ai_response(f\"\")\n",
        "#print(\"AI:\", ai_response)\n",
        "speak(\"Hi! Good evening. I am SalesGenie, a virtual Salesman. Am I speaking with Rohan and is this a good time to speak?\")\n",
        "# Wait until speech is finished\n",
        "time.sleep(6)\n",
        "audio = record_audio()\n",
        "user_input = transcribe(audio)\n",
        "ai_response = get_ai_response(f\"You are a sales assistant. You are calling to talk about {context}. The person has responded he is available to talk. Get the person onboard with the idea\")\n",
        "speak(ai_response)\n",
        "\n",
        "while True:\n",
        "    # Record audio\n",
        "    audio = record_audio()\n",
        "\n",
        "    user_input = transcribe(audio)\n",
        "\n",
        "    print(\"Transcribed text:\", user_input)\n",
        "\n",
        "    ai_response = get_ai_response(f\"You are a sales assistant. You are calling to talk about {context}. Respond to: {user_input}\")\n",
        "    print(\"AI:\", ai_response)\n",
        "    speak(ai_response)\n",
        "\n",
        "    if \"goodbye\" in user_input.lower():\n",
        "        speak(\"Thank you for your time. Goodbye!\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "OYYB2pKzZ9EE",
        "outputId": "fec26581-9b90-4f51-f1eb-071d49f1b98a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ApiError",
          "evalue": "status_code: 401, body: {'detail': {'status': 'quota_exceeded', 'message': 'This request exceeds your quota of 10000. You have 35 credits remaining, while 113 credits are required for this request.'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mApiError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-e8602be8e17e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#ai_response = get_ai_response(f\"\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#print(\"AI:\", ai_response)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mspeak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hi! Good evening. I am SalesGenie, a virtual Salesman. Am I speaking with Rohan and is this a good time to speak?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# Wait until speech is finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-f49bff7eb36d>\u001b[0m in \u001b[0;36mspeak\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0maudio_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maudio_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0maudio_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/elevenlabs/text_to_speech/client.py\u001b[0m in \u001b[0;36mconvert_as_stream\u001b[0;34m(self, voice_id, text, enable_logging, optimize_streaming_latency, output_format, model_id, language_code, voice_settings, pronunciation_dictionary_locators, seed, previous_text, next_text, previous_request_ids, next_request_ids, use_pvc_as_ivc, apply_text_normalization, request_options)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mApiError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mApiError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_response_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     def stream_with_timestamps(\n",
            "\u001b[0;31mApiError\u001b[0m: status_code: 401, body: {'detail': {'status': 'quota_exceeded', 'message': 'This request exceeds your quota of 10000. You have 35 credits remaining, while 113 credits are required for this request.'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\""
      ],
      "metadata": {
        "id": "SCveijg6LtaL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9d2a1576-a2fa-4afa-e2bf-a8468eb3d95b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ukdeuqiPNwGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main conversation loop\n",
        "print(\"Hello! I'm a virtual sales assistant. Are you available to talk?\")\n",
        "\n",
        "while True:\n",
        "    # Here you would record audio, but for simplicity, we'll use text input\n",
        "    user_input = input(\"Your response (text for now): \")\n",
        "\n",
        "    # In a real implementation, you'd use:\n",
        "    #audio_file = record_audio()\n",
        "    #user_input = transcribe(audio_file)\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "      model=\"gemini-2.0-flash\", contents=f\"You are a sales assistant. Respond to: {user_input}\")\n",
        "    print(response.text)\n",
        "\n",
        "    #speak(ai_response)\n",
        "\n",
        "    if \"goodbye\" in user_input.lower():\n",
        "        speak(\"Thank you for your time. Goodbye!\")\n",
        "        break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "6REnpoYG3usg",
        "outputId": "15e7bf2b-f90b-48ee-cb1c-55138fffcf39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I'm a virtual sales assistant. Are you available to talk?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-b0a8846b2b18>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Here you would record audio, but for simplicity, we'll use text input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your response (text for now): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# In a real implementation, you'd use:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CemlHyEqcfZw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}